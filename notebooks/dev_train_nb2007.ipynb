{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dev_train_nb2007.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lrdS2D_6Wpbn","colab_type":"text"},"source":["# Set-up"]},{"cell_type":"code","metadata":{"id":"81qVPggxUS-M","colab_type":"code","colab":{}},"source":["!pip install transformers torch pandas sklearn seqeval gdown"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izBHFgEwQjg-","colab_type":"code","colab":{}},"source":["import os\n","\n","if not os.path.exists(\"test_data\"):\n","    os.makedirs(\"test_data\")\n","\n","import gdown\n","\n","url = 'https://drive.google.com/uc?id=1Z32tmKPjIVkHm88MWjirdNqPfeUgqfxp'\n","output = 'test_data/wnut17train.conll'\n","gdown.download(url, output, quiet=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"et9eLV_jVfKp","colab_type":"code","colab":{}},"source":["from pathlib import Path\n","import re\n","from sklearn.model_selection import train_test_split\n","from transformers import DistilBertTokenizerFast\n","import numpy as np\n","import torch\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LRRBVj2raUrL","tags":[],"colab":{}},"source":["def read_conll(file_path):\n","    \"\"\"\n","    Reads .conll file which has a new token and its entity on each line. Documents are split by an empty line.\n","\n","    Parameters:\n","    file_path (string): Path to .conll file\n","\n","    Returns:\n","    token_docs (list): contains lists of strings, each list of each documents of tokens\n","    tag_docs (list): contains lists of strings, each list of each documents of entity tags\n","\n","    \"\"\"\n","\n","    # read file from path\n","    file_path = Path(file_path)\n","    raw_text = file_path.read_text().strip()\n","\n","    # split documents\n","    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n","\n","    # iterate over docs to extract tokens and tags\n","    token_docs = []\n","    tag_docs = []\n","    for doc in raw_docs:\n","        tokens = []  # holds a list for each doc of tokens\n","        tags = []  # holds a list for each doc of tags\n","        for line in doc.split('\\n'):\n","            token, tag = line.split('\\t')\n","            tokens.append(token)\n","            tags.append(tag)\n","        token_docs.append(tokens)\n","        tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n","\n","\n","def encode_tags(tags, encodings, tag2id):\n","    \"\"\"\n","    Encodes tags to corresponding ids and adjusts offsetting.\n","\n","    Parameters:\n","    tags (list): contains a list of tags of every token for each document\n","    encodings (object 'transformers.tokenization_utils_base.BatchEncoding'): \n","                encodings returned by the tokenizer of each document,\n","                is a dictionary containing keys input_ids, attention_mask, offset_mapping\n","    tag2id (dic): mapping of each tag to its id\n","\n","    Returns\n","    encoded_labels (list): contains a list of tag ids for each document padded with -100 for tokens that arent the first part of the original token.\n","    \"\"\"\n","\n","    # encode tags to ids\n","    tag_ids = [[tag2id[tag] for tag in doc] for doc in tags]\n","\n","    # adjust for offsetting\n","    labels_encoded = []\n","    for labels_i, offset_i in zip(tag_ids, encodings[\"offset_mapping\"]):\n","        # create an empty array of -100\n","        labels_encoded_i = np.ones(len(offset_i), dtype=int) * -100\n","        array_offset_i = np.array(offset_i)\n","\n","        # -- Everything that starts with 0 and ends with non-zero number is first part of original token.\n","        # -- Everything that starts and ends with 0 is a special token like [PAD] or [CLS] or [SEP].\n","        # set labels whose first offset position is 0 and the second is not 0\n","        labels_encoded_i[(array_offset_i[:, 0] == 0) & (\n","            array_offset_i[:, 1] != 0)] = labels_i  # select all indexes with starts with 0 and doesnt end with 0\n","        labels_encoded.append(labels_encoded_i.tolist())\n","\n","    return labels_encoded\n","\n","\n","def understand_offset(train_texts, train_tags, train_encodings, tokenizer):\n","    \"\"\"\n","    Conclusions: \n","    - (0,0) is special tokens for start and end sequence\n","    - (0, not0) is begin token of original token\n","    - (not0, 0) is follow-up token of original token\n","    \"\"\"\n","    print()\n","    print(\"-\"*100)\n","\n","    offset_text = train_encodings[\"offset_mapping\"][0]\n","    input_ids = train_encodings[\"input_ids\"][0]\n","    attention_mask = train_encodings[\"attention_mask\"][0]\n","\n","    text = train_texts[0]\n","    joined_text = \" \".join(text)\n","    tokenized_text = tokenizer.tokenize(\" \".join(text))\n","    tokenized_text = [\"BEGIN_TOKEN\"]+tokenized_text + \\\n","        [\"END_TOKEN\"] + [0 for x in attention_mask if x == 0]\n","\n","    print(\"Text: '{}'\".format(joined_text))\n","    print(\"--\")\n","\n","    count = 0\n","    for text, offset, input, attention in zip(tokenized_text, offset_text, input_ids, attention_mask):\n","        if not str(offset) == '(0, 0)' and '(0, ' in str(offset):\n","            count += 1\n","        print(\"Text = {}\".format(text))\n","        print(\"Offset = {}\".format(offset))\n","        print(\"Input ID = {}\".format(input))\n","        print(\"Attention Mask = {}\".format(attention))\n","        print(\"--\")\n","\n","    print(\"Number of tags = {} and number of (0,0) offsets = {}\".format(\n","        len(train_tags[0]), count))\n","    print(\"-\"*100)\n","    print()\n","\n","\n","class EEDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n","\n","    __len__ so that len(dataset) returns the size of the dataset.\n","    __getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n","\n","    Parameters:\n","    encodings (dict): containing input_ids and attention_mask for each doc\n","    labels (list): containing lists of labels for each token\n","    \"\"\"\n","\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    # returns a dictionary containing values of input_id, attention_mask and label for index 'idx'\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx])\n","                for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n","def create_dataset(file_path, verbose=False):\n","    \"\"\"\n","    Creates training and valuation datasets for training. Steps are:\n","    1. Extract lists of tags and docs using read_conll()\n","    2. Perform train-val split using sklearn\n","    3. Create set of unique tags\n","    4. Create tag ids + dictionarie tag2id and id2tag\n","    5. Load tokenizer from pretrained HF Tokenizers, returns offset mapping to be able to adjust tags\n","    6. Create encodings for tokens: uses tokenizer to create input ids and attentionmask\n","    7. Adjust tags for offsettting using encode_tags()\n","\n","    Returns:\n","    train_dataset (object of EEDataset)\n","    train_tags (list)\n","    val_dataset (object of EEDataset)\n","    val_tags (list)\n","    unique_tags (set)\n","    id2tag (dict)\n","    \"\"\"\n","\n","    print(\"-- Reading data..\")\n","    texts, tags = read_conll(file_path)\n","\n","    print(\"-- Train-val split data..\")\n","    train_texts, val_texts, train_tags, val_tags = train_test_split(\n","        texts, tags, test_size=.2)\n","\n","    print(\"-- Creating set of unique tags..\")\n","    unique_tags = set(tag for doc in tags for tag in doc)\n","\n","    print(\"-- Creating dictionary of unique tag 2 ids..\")\n","    tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n","\n","    print(\"-- Creating dictionary of unique ids 2 tags..\")\n","    id2tag = {id: tag for tag, id in tag2id.items()}\n","\n","    print(\"-- Loading tokenizer..\")\n","    tokenizer = DistilBertTokenizerFast.from_pretrained(\n","        'distilbert-base-cased')\n","\n","    print(\"-- Creating encodings..\")\n","    train_encodings = tokenizer(train_texts, is_pretokenized=True,\n","                                return_offsets_mapping=True, padding=True, truncation=True)\n","    val_encodings = tokenizer(val_texts, is_pretokenized=True,\n","                              return_offsets_mapping=True, padding=True, truncation=True)\n","\n","    if verbose:\n","        understand_offset(train_texts, train_tags, train_encodings, tokenizer)\n","\n","    print(\"-- Adjusting labels for offsetting ..\")\n","    train_tag_ids_offadj = encode_tags(\n","        train_tags, train_encodings, tag2id)\n","    val_tag_ids_offadj = encode_tags(val_tags, val_encodings, tag2id)\n","\n","    # remove offset mapping now it is no longer needed\n","    train_encodings.pop(\"offset_mapping\")\n","    val_encodings.pop(\"offset_mapping\")\n","\n","    # Create dataset\n","    print(\"-- Creating dataset..\")\n","    train_dataset = EEDataset(\n","        encodings=train_encodings, labels=train_tag_ids_offadj)\n","    val_dataset = EEDataset(encodings=val_encodings,\n","                            labels=val_tag_ids_offadj)\n","\n","    return train_dataset, train_tags, val_dataset, val_tags, unique_tags, id2tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgzILrFcVZNE","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","from transformers import DistilBertForTokenClassification, Trainer, TrainingArguments\n","from torch import nn\n","from seqeval.metrics import f1_score, classification_report\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXh-pR6NVVjO","colab_type":"code","colab":{}},"source":["import os\n","from ast import literal_eval\n","import shutil\n","\n","def compute_metrics(pred):\n","\n","    f = open(\"_temp/id2tag.txt\", \"r\")\n","    content = f.read()\n","    f.close()\n","\n","    id2tag = literal_eval(content)\n","    \n","\n","    label_ids = pred.label_ids\n","    preds = np.argmax(pred.predictions, axis=2)\n","    batch_size, seq_len = preds.shape\n","\n","    preds_list = [[] for _ in range(batch_size)]\n","    label_list = [[] for _ in range(batch_size)]\n","    for i in range(batch_size):\n","        for j in range(seq_len):\n","            # ignore pad_tokens\n","            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n","                preds_list[i].append(id2tag[preds[i][j]])\n","                label_list[i].append(id2tag[label_ids[i][j]])\n","\n","\n","    results = dict()\n","\n","    f1 = f1_score(label_list, preds_list)\n","    results[\"f1\"] = f1\n","\n","    rep = classification_report(label_list, preds_list).split('\\n')\n","    par_names = rep[0].split(\" \")\n","    par_names = [x for x in par_names if x != '']\n","\n","    rep = rep[2:(len(rep)-4)]\n","    for r in rep:\n","        r = r.split(\" \")\n","        r = [x for x in r if x != '']\n","        ee_name = r[0]\n","        r = r[1:]\n","\n","        for idx, par_name in enumerate(par_names):\n","            results[ee_name+\"_\"+par_name] = float(r[idx])\n","\n","    return results\n","\n","def main(file_path):\n","    train_dataset, train_tags, val_dataset, val_tags, unique_tags, id2tag = create_dataset(\n","        file_path=file_path)\n","\n","    data = train_dataset, train_tags, val_dataset, val_tags, unique_tags, id2tag\n","\n","    if os.path.exists(\"_temp\"):\n","        shutil.rmtree(\"_temp\")\n","    \n","    os.makedirs(\"_temp\")\n","    f = open(\"_temp/id2tag.txt\", \"w\")\n","    f.write(str(id2tag))\n","    f.close()\n","\n","\n","    print(\"-- Loading model..\")\n","    model = DistilBertForTokenClassification.from_pretrained(\n","        'distilbert-base-cased', num_labels=len(unique_tags))\n","\n","    training_args = TrainingArguments(\n","        output_dir='./results',          # output directory\n","        num_train_epochs=3,              # total number of training epochs\n","        per_device_train_batch_size=16,  # batch size per device during training\n","        per_device_eval_batch_size=64,   # bxatch size for evaluation\n","        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","        weight_decay=0.01,               # strength of weight decay\n","        evaluate_during_training=True,\n","        logging_dir='./logs',            # directory for storing logs\n","        logging_first_step = True,\n","        logging_steps = 250,\n","        eval_steps = 250, \n","        save_steps = 1000\n","    )\n","\n","    trainer = Trainer(\n","        # the instantiated ðŸ¤— Transformers model to be trained\n","        model=model,\n","        args=training_args,                  # training arguments, defined above\n","        compute_metrics=compute_metrics,\n","        train_dataset=train_dataset,         # training dataset\n","        eval_dataset=val_dataset             # evaluation dataset\n","    )\n","\n","    print(\"-- Start training..\")\n","    trainer.train()\n","\n","    #print(\"-- Start evaluation.. \")\n","    #trainer.evaluate()\n","\n","    shutil.rmtree(\"_temp\")\n","    return trainer, data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vty75z_pWgAK","colab_type":"text"},"source":["# Training & Evaluation Script"]},{"cell_type":"code","metadata":{"id":"cvFc-lG5Qgzf","colab_type":"code","colab":{}},"source":["FILE_PATH = 'test_data/wnut17train.conll'\n","trainer, data = main(file_path=FILE_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kh4m-HVONwLl","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]}]}